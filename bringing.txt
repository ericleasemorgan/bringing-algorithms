

Bringing algorithms and machine learning into library collections & services


Abstract

Many aspects of librarianship have been automated to one degree or another. Now, in a time of "big data", is it possible to go beyond mere automation and towards the more intelligent use of computers; the use of algorithms and machine learning is an integral part of future library collection building and service provision. To make the point, this essay first highlights a number of changes in librarianship that were deemed revolutionary in their time but are now taken for granted. Second, this essay compares & contrasts library automation with the "intelligent" use of computers. Finally, this essay introduces an application/system called the Distant Reader which puts some of these ideas into practice. Librarianship is evolutionary. Algorithms and machine learning are not librarian replacements, but instead, they are additional tools in the professional toolbox. This essay outlines how & why. 


Seemingly revolutionary changes

At the time of their implementation, some changes in the practice of librarianship were deemed revolutionary, but now-a-days some of these same changes are deemed matter of fact. Take, for example, the idea of the catalog. For a long time a library catalog was merely an acquisitions list. Given some additional characteristics such as authors and topics, these acquisitions lists were manifested as books, books which could be mass printed and distributed. But the books were difficult to keep up to date, and they were expensive to print. As a consequence, catalog cards were invented, and the catalog became a massive set of drawers. Unfortunately, because the way catalog cards were produced, it is not feasible to assign more than three or four subject headings to any given book. If one does, then the number of catalog cards quickly gets out of hand. Also, the idea of sharing catalog cards between libraries was common, and the Library of Congress facilitated much of this work. With the advent of computers the idea of sharing cataloging data as MARC (machine readable cataloging) became prevalent. The data structure of a MARC record is indicative of the time. Intended to be distributed on reel-to-reel tape, the MARC record is a sequencial data structure designed to be read from beginning to end, and all along there way there are checks & balances insuring the records' integrity. Despite the apparent flexibility of a digital data structure, the tradition of three for four subject headings per book still holds true. Now-a-days, the data from MARC records is used to fill databases, the databases' content is indexed, and items are located by searching the index. The evolution of the venerable library catalog has spanned centuries, and each evolutionary change was seen a revolutionary. Moreover, each evolutionary change solved some problems but created new ones. None of the changes were cure-alls. [HISTORY]

With the advent of the Internet, a host of other changes are happening in Library Land. Some of them are seen as revolutionary, and only time will tell whether or not these changes will persevere. Examples include but are not limited to:

  * the creation & maintenance of institutional repositories
  * the increasing tendency to license content
  * the continuing dichotomy of the virtual library & library as place
  * the existence of digital scholarship centers


Working smarter, not harder

It is ironic. Despite the fact that libraries have the world of knowledge at their fingertips, libraries do not operate very intelligently, but such a bold statement needs context. The application of computer technology to the practice of librarianship has almost exclusively been about automation, and it has not exploited computer technology.

Let's enumerate the core functionalities of computers. First of all, computers... compute. They are given some sort of input, assign the input to a variable, apply any number of functions to the variable, and output the result. This process -- computing -- is akin to solving simple algebraic equations such as calculating the area of a circle or estimating a distance traveled. There are two things of particular interest here. First, the input can be as simple as a number or a string (read "a word") or the input can be arbitrarily large combinations of both. Examples include:

  * 42
  * 1776
  * xyzzy
  * George Washington
  * a MARC record
  * the circulation history and academic characteristics of an individual
  * the full text and bibliographic descriptions of all Charles Dickens's works

What is really important here is the possible scale of a computer's input. Libraries have not taken advantage of that scale. Imagine how librarianship would change if the profession actively used the full text its collections to enhance bibliographic description and public service. Imagine how collection policies and patron needs could be better articulated if the totality of circulation histories and journal usage histories where thoroughly investigated, especially if the histories of other libraries were included.

Second, when it comes to computing, it involves "function calls", subroutines which are used over and over in a computer program. These function calls take an input and return a result much like a tiny computer program. Computer programmers save sets of function calls (subroutines) as files. As files of subroutines accumulate the collection was often called a "library". The metaphor works well because the files can then be "checked out" of the library and included in other computer programs. What's more, this is an ideal library because programmers do not need to return the materials, and consequently there were no overdue notices. What is reallhy important about function calls is that they build upon themselves in a manner akin to Euclid's Elements. Function calls can be combined to build more and more complex systems based on very tiny components.

A second core functionality of computers are their ability to save, organize, and retrieve vast amounts of data. Like the functions that build upon themselves, the data computers save, organize, and retrieve can be compounded.

More specifically, computers save "data" -- mere numbers and strings. But when the data is given context, such as a number denoted as date or a string denoted as a name, then the data is transformed into information. An example might include the year 1972 and the name of Blake. Given additional information which may be compared & contrasted to other information, knowledge can be created -- information put to use and understood. For example, Mary was born in 1951 and therefore she is 21 years older than Blake. Computers excel at saving, organizing, and retrieving data which leads to information and knowledge. The possibilities of computers dispensing wisdom  -- knowlege of a timeless nature -- is left for another essay.

Like the scale of computer input, the library profession has not really exploited computers' ability to save, organize, and retrieve data, despite the fact that such things are at the core of librarianship. Put another way, on the whole, the library profession does not understand the concept of a "data structure". Again, data becomes information when it is given context. In the world of MARC, when a string ("words") is inserted into the 245 field of a MARC bibliographic record, then the string magically becomes a title. In this case, MARC is a "data structure" because different fields denote different contexts. There are fields for authors, subjects, notes, added entries, etc. This is all very well and good, especially considering when MARC was designed more than fifty years ago. But since then much more scalable, flexible, and efficient data structures have been articulated.

Relational databases are a very good example. Relational databases build on a classic data structure known as the "table" -- a matrix of rows and columns where each row is a record and each column is a field. Think "spreadsheet". For example, each row may represent a book, and there are columns for authors, titles, dates, publishers, etc. The problem comes when a column may need to be repeatable. For example, a book may have multiple authors or more commonly, multiple subjects. In this case the idea of a table breaks down because it doesn't make sense to have a column named subject-01, subject-02, and subject-03. As soon as you do that, you will want subject-04. Relational database solve this problem. The solution is to first add a "key" -- a unique value -- to each row. Next, for fields with multiple values, create a new table is created where one of the columns is the key from the first table and the other column is a value, in this case, a subject heading. There are now two tables and they can be "joined" through the use of the key. Given such a data structure it is possible to add as many subjects as desired to any bibliographic item.

But you say, "MARC can handle multiple subjects." True, MARC can handle multiple subjects, but underneath, MARC is a data structure designed when information was disseminated on tape. As such, it is a sequencial data format intended to be read from beginning to end. It is not a random access structure. What's more, the MARC data structure is really divided into three substructures: 1) the leader, which is always twenty-four characters long, 2) the directory, which denotes where what bibliographic fields exist and where they are located, and 3) the bibliographic section where the bibliographic information is actually stored. It gets more complicated. The first five characters of the leader is expected to be left-hand, zero-padded integer denoting the length) of the record measured in bytes. A typical value may be 01999. Thus, the record is 1999 bytes long. Now, ask yourself, "What is the maximum size of MARC record?" Despite the fact that librarianship embraces the idea of MARC, very few librarians really and truely understand the structure of MARC data. MARC is a format for transmitting data from one place to another, not organization. As a whole, the library profession does not understand the concept of a relational database, despite the fact that it is a superior way of organizing data and turning thus turning it into information.

Moreover, there is more information in libraries besides bibliographic information. There is information about people and organizations. Information about resource usage. Information about licensing. Information about resources that are not bibliographic, such as images or data sets. Etc. When these types of information present themselves, libraries fall back to the use of simple tables, which are usually not really amenable for turning data into information.

There are many different data structures. XML became popular about twenty years ago. Since then JSON has become prevalent. More than twenty years ago the idea of the Semantic Web and Linked Data presented themselves. All of these data structures have various strengths and weaknesses. None of them is perfect, and each addresses different needs, but they are all better than MARC when it comes to organizing data.

Libraries understand the concept of manifesting data as information, but as a whole, libraries do not know how to manifest the concept using computer technology.

Finally, another core functionality of computers is networking and communication. The advent of the Internet is a relatively recent phenomenon, but the all but ubiquitous nature of computers and other "smart" devices combined with the Internet has literally facilitated billions of connections between computers (and people). Consequently the data computed upon and stored in one place can be transmitted almost instantly to another place. Moreover, the transmission is an exact copy of the original.

Again, like the process of computing and the process of storage, efficient computer communication builds upon itself with foreseen consequences. For example, who foresaw the demise of many centralized information authorities? For example, with the advent of the Internet there is less of a need/desire for travel agents, movie reviewers, or dare I say it, libraries providing their traditional services.

Yet again, libraries use the Internet, but do they actually exploit it? How many librarians are able to create a file, of any type, put it on the Web, and then share the resulting URL with whomever? Granted, centralized computing departments and networking administrators put up road blocks to doing such things, but the sharing of data and information is at the core of librarianship. Putting a file on the 'Net, even temporarily, t is something every librarian ought to be able to know how (and be authorized) to do. 

Despite the functionality of computers and their place in libraries over the past fifty to sixty years, computers have mostly been used to automate library tasks. MARC automated the process of printing catalog cards and eventually the creation of "discovery systems". Libraries have used computers to automate the process of lending materials between themselves as well as to local learners, teachers, and scholars. Libraries use computers to store, organize, preserve, and disseminate the gray literature of our time, and we call these systems "institutional repositories". In all of these cases, the automation has been a good thing because efficiencies were gained, but the use of computers has not gone far enough nor really evolved. Libraries have not truly exploited the functionality of computers. Lending and usage statistics are not routinely harvested nor organized for the purposes of monitoring nor predicting library patron needs/desires. The content of institutional repositories is usually born digital, but libraries have not exploited the full text nature nor created services going beyond rudimentary catalogs.

Libraries have not really embraced computer technology. Computers can do so much more for libraries than mere automation. While I will never say computers are "smart", their fundamental characteristics do appear intelligent, especially when used at scale. The scale of computing has significantly changed for the past ten years, and with this change the concept of "machine learning" has become more feasible. The following sections outlines how libraries can go beyond automation, embrace machine learning, and truly evolve its ideas of collections and services. 


Machine learning

Machine learning is a computing process used to make decisions and predictions.

In the past, compter-aided decision-making and predictions where accomplished by articulating large sets of if-then statements and navigating down a decision tree. The applications were extremely domain specific, and they weren't very scalable. Machine learning turns this process on its head. Instead of navigating down a tree, machine learning takes sets of previously made observations (think "decisions"), identifies patterns and anomalies in the observations, and saves the result as a mathematical model, which is really an n-dimensional array of vectors.  Observations outside the given observations are then compared to the model and depending on the resulting similarities or differences, decisions or predictions are drawn.

Using such a process, there are really only four different types of machine learning: classification, clustering, regression, and dimension reduction. Classification is a supervised machine learning process used to subdivide a set of observations into smaller sets which have been previously articulated. For example, suppose you had a few  categories of restaurants such as American, French, Italian, or Chinese. Given a set of previously classified menus, one could create a model defining each category and then classify new, unseen menus. The classic classification example is the filtering of email. "Is this message 'spam' or 'ham'?" This chapter's appendix walks a person through the creation of a simplified classification system. It classifies texts based on authorship. 

Clustering is an unsupervised machine learning process which also creates smaller sets from a larger one, but clustering is not given a set of previously articulated categories. That is what makes it "unsupervised". Instead, the categories are created as an end result. Topic modeling is an example of clustering.

Regression predicts a numeric value based on sets of dependent variables. For example, given dependent variables like annual income, education level, size of family, age, gender, religion, and employment status, then one might predict how much money a person may spend on an an independent variale such as charity.

Sometimes the number of characteristics of each observation is very large. Many times some of these characteristics do not play a significant role in decision-making nor prediction. Dimension reduction is another machine learning process, and it is used to eliminate these less-then-useful characteristics from the observations thus making classification, clustering, and regression easier. 

There are many ways to enhance library collections and services through the use of machine learning. I'm not necessarily advocating the implementation of any of the following ideas, but they are possibilities. Each is grouped into the broadest of library functional departments:


  * reference and public services

   o given a set of grant proposals, suggest library resources to be used in support of the grants

   o given a set of licensed library resources and their usage, suggest other resources for use

   o given a set of previously checked out materials, suggest other materials to be checked out

   o given a set of reference interviews, create a chatbot to supplement reference services

   o given the full text of a set of desirable journal articles, create a search strategy to be applied against any number of bibliographic indexes; answer the proverbial question, "Can you help me find more like this one?"

   o given the full text of articles as well as their bibliographic descriptions, predict and describe the sorts of things a specific journal title accepts or whether a given draft is good enough for publication

   o given the full text of reading materials assigned in a class, suggest library resources to support them


  * technical services

    o  given a set of multimedia, enumerate characteristics of the media (number of faces, direction of angles, number and types of colors, etc.), and use the results to supplement bibliographic description

    o given a set of previously cataloged items, determine whether or not the cataloging can be improved

    o given full-text content harvested from just about anywhere (the Web, your institutional repository, transcriptions from your special collections department, etc.), analyze the content in terms of natural language processsing, and supplement the bibliographic descriptions


  * collections
  
    o given circulation histories, articulate more refined circulation patterns, and use the results to refine collection development policies

    o given the full text of sets of theses and disserations, predict where scholarship at your institution is growing, and use the results to more intelligently build your just-in-case collection; do the same thing with faculty publications

Here at the University of Notre Dame's Navari Center for Digital Scholarship, we use machine learning in a number of ways. We cut our teeth on a system called Convocate. [CONVOCATE] In this case we obtained a set of literature on the theme of human rights. Half of the set was written by researchers in non-governmental organizations. The other half was written by theologians. While both sets were on the same theme, the language of each was different. An excellent example was the use of the word "child". In the former case, children were included with fathers and mothers. In the later case, children often refereed to the "Children of God". Consequently, queries with the word children were often misleading. To rectify this problem, a set of broad themes were articulated, such as Actors, Harms and Violations, Rights and Freedoms, and Principles and Values. We then used topic modeling to subdivide all of the paragraphs of all of the documents into smaller and smaller sets of paragraphs. We then compared the resulting topic to the broad themes, and when we found a correlation between the two we classified the paragraphs accordingly. In retrospect, this process was not ideal, but we were learning and the resulting index is useful.

On a regular basis we find ourselves using a program called Topic Modeling Tool, which is based on the venerable MALLET application. [TOOL, MALLET]

We have used classification techniques in at least a couple of ways.


Distant Reader

The use of machine learning need not be the end result of a computing process, but rather a means to an end. Such is the case when it comes to a high performance application/system called the Distant Reader. The Distant Reader uses a few machine learning techniques as well as natural language processes to to supplement the traditional reading process.


Appendix A: Train and classify

This appendix lists two Python programs. The first (train.py) creates a model for the classification of plain text files. The second (classify.py) uses the output of the first to classify new plain text files. For your convenience, the scripts and some sample data ought to be available on GitHub at https://github.com/ericleasemorgan/bringing-algorithms, but eventually this link will break. The purpose of including these two script is to help demystify the process of machine learning; a machine learning process can be rather simple.


Train

The following Python script is a simplistic classification training application.

Given a file name and a list of directories containing .txt files, this script first reads all the files' contents and the names of their directories into sets of datum and labels (think "categories"). It then divides the data and labels into training and testing sets. Such is a best practice for these types of programs so the models can be evaluated for accuracy. Next, the script counts & tabulates ("vectorizes") the training data and creates a model of the data using a variation of the Naive Bayes algorithm. The script then counts & tabulates the test data, uses the model the classify the test data, and compares the resulting classifications to the originally supplied labels. The result is an accuracy score, and generally speaking, a score greater than 75% is on the road to success. A score of 50% is no better than flipping a coin. Finally, the model is saved to a file for later use.


# train.py - given a file name and a list of directories containing .txt files, create a model for classifying similar items

# require the libraries/modules that will do the work
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection         import train_test_split
from sklearn.naive_bayes             import MultinomialNB
import glob, os, pickle, sys

# sanity check; make sure the program has been given input
if len( sys.argv ) < 4 :
	sys.stderr.write( 'Usage: ' + sys.argv[ 0 ] + " <model> <directory> <another directory> [<another directory> ...]\n" )
	quit()

# get the name of the file where the model will be saved
model = sys.argv[ 1 ]

# get the rest of the input, the names of directories to process
directories = []
for i in range( 2, len( sys.argv ) ) : directories.append( sys.argv[ i ] )

# initialize the data to analyze and its associated labels
data   = []
labels = []

# loop through each given directory
for directory in directories :

	# find all the text files and get the directory's name
	files = glob.glob( directory + "/*.txt" )
	label = os.path.basename( directory )
	
	# process each file
	for file in files :
		
		# open the file
		with open( file, 'r' ) as handle :
		
			# add the contents of the file to the data
			data.append( handle.read() )
			
			# update the list of labels
			labels.append( label )

# divide the data/labels into training sets and testing sets; a best practice
data_train, data_test, labels_train, labels_test = train_test_split( data, labels )

# initialize a vectorizer, and then count/tabulate the training data
vectorizer = CountVectorizer( stop_words='english' )
data_train = vectorizer.fit_transform( data_train )

# initialize a classification model, and then use Naive Bayes to create a model
classifier = MultinomialNB()
classifier.fit( data_train, labels_train )

# count/tabulate the test data, and use the model to classify it
data_test       = vectorizer.transform( data_test )
classifications = classifier.predict( data_test )

# begin to test for accuracy
count = 0

# loop through each test classification
for i in range( len( classifications ) ) :
    
    # increment, if the calculated classification match the given classification
    if classifications[ i ] == labels_test[ i ] : count += 1

# calculate and output the accuracy score; above 75% begins to achieve success
print ( "Accuracy: %s%% \n" % ( int( ( count * 1.0 ) / len( classifications ) * 100 ) ) )

# save the vectorizer and the classifier (the model) for future use, and done
with open( model, 'wb' ) as handle : pickle.dump( ( vectorizer, classifier ), handle )
exit()


Classify

The following Python script is a simplistic classification program.

Given the model created by the previous script (train.py) and a directory containing a set of .txt files, this script will output a suggested label ("classification") and a file name for each file in the given directory. This script automatically classifies a set of plain text files.


# classify.py - given a previously saved classification model and a directory of .txt files, classify a set of documents

# require the libraries/modules that will do the work
import glob, os, pickle, sys

# sanity check; make sure the program has been given input
if len( sys.argv ) != 3 :
	sys.stderr.write( 'Usage: ' + sys.argv[ 0 ] + " <model> <directory>\n" )
	quit()

# get input; get the model to read and the directory containing the .txt files
model     = sys.argv[ 1 ]
directory = sys.argv[ 2 ]

# read the model
with open( model, 'rb' ) as handle : ( vectorizer, classifier ) = pickle.load( handle )

# process each .txt file
for file in glob.glob( directory + "/*.txt" ) :
	
	# open, read, and classify the file
	with open( file, 'r' ) as handle : classification = classifier.predict( vectorizer.transform( [ handle.read() ] ) )
	
	# output the classification and the file's name
	print( "\t".join( ( classification[ 0 ], os.path.basename( file ) ) ) )
	
# done
exit()


Notes and links

[HISTORY] For quick and easy-to-read histories of library catalogs and MARC see ["Catalogs, their history" and "MARC for library use"].

[CONVOCATE] https://convocate.nd.edu

[TOOL] https://github.com/senderle/topic-modeling-tool

[MALLET] http://mallet.cs.umass.edu

